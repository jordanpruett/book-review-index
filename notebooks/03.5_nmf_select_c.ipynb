{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I select the optimal number of latent features C, the hyperparameter used in our nonnegative matrix factorization notebook. \n",
    "\n",
    "I used the method described by Alex Williams here: http://alexhwilliams.info/itsneuronalblog/2018/02/26/crossval/\n",
    "\n",
    "This notebook is copied from an implementation by Ishay Telavivi (with minor tweaks) that is available here: https://github.com/IshayTelavivi/cross_validation_matrix_completion/blob/master/cv_for_nmf_sample1.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import linalg\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to load the data and preprocess it in the same way we will in the final version. This involves grouping on authors, then weighting the counts with a chosen weighting scheme (TF-IDF or PMI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Journal of American History</th>\n",
       "      <th>Humanist</th>\n",
       "      <th>Sewanee Review</th>\n",
       "      <th>American City and Council</th>\n",
       "      <th>Encounter</th>\n",
       "      <th>Parnassus: Poetry in Review</th>\n",
       "      <th>Instructor</th>\n",
       "      <th>Review of Metaphysics</th>\n",
       "      <th>American Anthropologist</th>\n",
       "      <th>Harvard Law Review</th>\n",
       "      <th>...</th>\n",
       "      <th>American Forests</th>\n",
       "      <th>Church History</th>\n",
       "      <th>Parks and Recreation</th>\n",
       "      <th>English Journal</th>\n",
       "      <th>British Book News</th>\n",
       "      <th>Philosophical Review</th>\n",
       "      <th>Western Humanities Review</th>\n",
       "      <th>Journal of English and Germanic Philology</th>\n",
       "      <th>American Literature</th>\n",
       "      <th>Washington Monthly</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>author_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CARR, Rachel</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.441518</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>YAU, John</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.577147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SIMON, Roger L</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PERELMAN, S J</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.659276</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JACOBSON, Willard J</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>24.415180</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 352 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Journal of American History  Humanist  Sewanee Review  \\\n",
       "author_name                                                                  \n",
       "CARR, Rachel                                 0.0       0.0        0.000000   \n",
       "YAU, John                                    0.0       0.0        0.000000   \n",
       "SIMON, Roger L                               0.0       0.0        0.000000   \n",
       "PERELMAN, S J                                0.0       0.0        2.659276   \n",
       "JACOBSON, Willard J                          0.0       0.0        0.000000   \n",
       "\n",
       "                     American City and Council  Encounter  \\\n",
       "author_name                                                 \n",
       "CARR, Rachel                               0.0        0.0   \n",
       "YAU, John                                  0.0        0.0   \n",
       "SIMON, Roger L                             0.0        0.0   \n",
       "PERELMAN, S J                              0.0        0.0   \n",
       "JACOBSON, Willard J                        0.0        0.0   \n",
       "\n",
       "                     Parnassus: Poetry in Review  Instructor  \\\n",
       "author_name                                                    \n",
       "CARR, Rachel                            0.000000    2.441518   \n",
       "YAU, John                               4.577147    0.000000   \n",
       "SIMON, Roger L                          0.000000    0.000000   \n",
       "PERELMAN, S J                           0.000000    0.000000   \n",
       "JACOBSON, Willard J                     0.000000   24.415180   \n",
       "\n",
       "                     Review of Metaphysics  American Anthropologist  \\\n",
       "author_name                                                           \n",
       "CARR, Rachel                           0.0                      0.0   \n",
       "YAU, John                              0.0                      0.0   \n",
       "SIMON, Roger L                         0.0                      0.0   \n",
       "PERELMAN, S J                          0.0                      0.0   \n",
       "JACOBSON, Willard J                    0.0                      0.0   \n",
       "\n",
       "                     Harvard Law Review  ...  American Forests  \\\n",
       "author_name                              ...                     \n",
       "CARR, Rachel                        0.0  ...               0.0   \n",
       "YAU, John                           0.0  ...               0.0   \n",
       "SIMON, Roger L                      0.0  ...               0.0   \n",
       "PERELMAN, S J                       0.0  ...               0.0   \n",
       "JACOBSON, Willard J                 0.0  ...               0.0   \n",
       "\n",
       "                     Church History  Parks and Recreation  English Journal  \\\n",
       "author_name                                                                  \n",
       "CARR, Rachel                    0.0                   0.0              0.0   \n",
       "YAU, John                       0.0                   0.0              0.0   \n",
       "SIMON, Roger L                  0.0                   0.0              0.0   \n",
       "PERELMAN, S J                   0.0                   0.0              0.0   \n",
       "JACOBSON, Willard J             0.0                   0.0              0.0   \n",
       "\n",
       "                     British Book News  Philosophical Review  \\\n",
       "author_name                                                    \n",
       "CARR, Rachel                       0.0                   0.0   \n",
       "YAU, John                          0.0                   0.0   \n",
       "SIMON, Roger L                     0.0                   0.0   \n",
       "PERELMAN, S J                      0.0                   0.0   \n",
       "JACOBSON, Willard J                0.0                   0.0   \n",
       "\n",
       "                     Western Humanities Review  \\\n",
       "author_name                                      \n",
       "CARR, Rachel                               0.0   \n",
       "YAU, John                                  0.0   \n",
       "SIMON, Roger L                             0.0   \n",
       "PERELMAN, S J                              0.0   \n",
       "JACOBSON, Willard J                        0.0   \n",
       "\n",
       "                     Journal of English and Germanic Philology  \\\n",
       "author_name                                                      \n",
       "CARR, Rachel                                               0.0   \n",
       "YAU, John                                                  0.0   \n",
       "SIMON, Roger L                                             0.0   \n",
       "PERELMAN, S J                                              0.0   \n",
       "JACOBSON, Willard J                                        0.0   \n",
       "\n",
       "                     American Literature  Washington Monthly  \n",
       "author_name                                                   \n",
       "CARR, Rachel                         0.0                 0.0  \n",
       "YAU, John                            0.0                 0.0  \n",
       "SIMON, Roger L                       0.0                 0.0  \n",
       "PERELMAN, S J                        0.0                 0.0  \n",
       "JACOBSON, Willard J                  0.0                 0.0  \n",
       "\n",
       "[5 rows x 352 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "df = pd.read_csv('../data/processed/book_reviews.tsv', sep='\\t', index_col=0)\n",
    "df['author_name'] = df.index.to_series().str.split('\\\\|\\\\|').str[1].str.strip()\n",
    "author_total_books = df['author_name'].value_counts()\n",
    "df = df.groupby('author_name').sum()\n",
    "df = df[df.index.notnull()]\n",
    "df = df.drop('#NAME?')\n",
    "\n",
    "# drop low-count authors and journals\n",
    "auth_min = 20\n",
    "journal_min = 25\n",
    "df = df[df.sum(axis=1) >= auth_min]\n",
    "df = df[df.columns[df.sum() >= journal_min]]\n",
    "df.shape\n",
    "\n",
    "# weight values\n",
    "docs = df.shape[0]\n",
    "idfs = [math.log(docs / np.where(df[col] == 0, 0, 1).sum()) for col in df.columns]\n",
    "tfidf = df * idfs\n",
    "\n",
    "# finally, shuffle the data\n",
    "tfidf = tfidf.sample(frac=1, random_state=99) # rows\n",
    "random.seed(99)\n",
    "shuffled_cols = list(tfidf.columns)\n",
    "random.shuffle(shuffled_cols)\n",
    "tfidf = tfidf[shuffled_cols] # columns\n",
    "\n",
    "tfidf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function splits the matrix into 4 sets for crossvalidation. In each one, 1/4 of the values are masked with zeroes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossval_matrices(X, fold):\n",
    "    \"\"\"\n",
    "    Given a matrix X, the function creates 4 sets of train + test matrices\n",
    "    where each train matrix is masked with zeros in 0.25 of the values, and the\n",
    "    test matrix is masked zeros in 0.75 of them.\n",
    "    X - numpy array\n",
    "    fold - is an integer from 0-3.\n",
    "    Returns the masked data and also the masks for train and test\n",
    "    \"\"\"\n",
    "    # Create a dict with the slicing indices\n",
    "    rows = X.shape[0]\n",
    "    cols = X.shape[1]\n",
    "    mid_rows = int(rows/2)\n",
    "    mid_cols = int(cols/2)\n",
    "    \n",
    "    idx_dict = {\n",
    "                0: [[0,mid_rows],[0, mid_cols]],\n",
    "                1: [[0,mid_rows],[mid_cols, cols]],\n",
    "                2: [[mid_rows, rows], [0, mid_cols]],\n",
    "                3: [[mid_rows, rows], [mid_cols, cols]]\n",
    "    }\n",
    "    \n",
    "    idexes = idx_dict[fold]\n",
    "    # Create masks\n",
    "    train_mask = np.full((rows, cols), 1)\n",
    "    train_mask[idexes[0][0]:idexes[0][1], idexes[1][0]:idexes[1][1]] = 0\n",
    "    test_mask = 1 - train_mask\n",
    "    \n",
    "    \n",
    "    # Create X_train\n",
    "    X_train = X.copy()\n",
    "    X_train[train_mask==0] = 0\n",
    "    \n",
    "    # Create X_test\n",
    "    X_test = X.copy()\n",
    "    X_test[train_mask==1] = 0\n",
    "        \n",
    "    return X_train, X_test, train_mask, test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# note that unlike the original, this version assumes no actual NaN values are in the data\n",
    "def nmf_crossval(X, latent_features, cycles=5, max_iter=500):\n",
    "    \"\"\"\n",
    "    Performs NMF with 1/4 of the data held out, decomposing matrix X to lower rank matrices W and H.\n",
    "    Calculates training and test error based on divergence of WH from training X and test (held-out) X.\n",
    "    \"\"\"\n",
    "    \n",
    "    # fold_tups is where we collect the attributes from each fold\n",
    "    fold_tups = []\n",
    "    \n",
    "    # Cross validation by 4 folds\n",
    "    for f in range(4):\n",
    "        X_train, X_test, train_mask, test_mask = crossval_matrices(X, f)\n",
    "        \n",
    "        # Get only the train values\n",
    "        masked_X = train_mask * X_train\n",
    "        model = NMF(n_components=latent_features, \n",
    "                        init='nndsvd', # nndsvd is faster with a sparse matrix\n",
    "                        random_state=99, \n",
    "                        max_iter=max_iter)\n",
    "        W = model.fit_transform(X_train)\n",
    "        H = model.components_\n",
    "        X_est = np.dot(W, H) # estimated reconstruction of source matrix\n",
    "        train_err = linalg.norm(train_mask * (X_train - X_est), ord='fro')\n",
    "        test_err = linalg.norm(test_mask * (X_test - X_est), ord='fro')\n",
    "\n",
    "        fold_tup = (W, H, train_err, test_err)\n",
    "        fold_tups.append(fold_tup)\n",
    "        \n",
    "        print('k: ', latent_features, ';  fold: ', f) \n",
    "        print('train residual', np.round(train_err, 4))\n",
    "        print('test residual', np.round(test_err, 4))\n",
    "        print()\n",
    "        \n",
    "    # Get avg train/test score from all folds\n",
    "    train_mean = np.mean([x[2] for x in fold_tups])\n",
    "    test_mean = np.mean([x[3] for x in fold_tups])\n",
    "    \n",
    "    return train_mean, test_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k:  2 ;  fold:  0\n",
      "train residual 1722.4158\n",
      "test residual 1450.3628\n",
      "\n",
      "k:  2 ;  fold:  1\n",
      "train residual 1677.9981\n",
      "test residual 1494.2566\n",
      "\n",
      "k:  2 ;  fold:  2\n",
      "train residual 1730.0207\n",
      "test residual 1432.9131\n",
      "\n",
      "k:  2 ;  fold:  3\n",
      "train residual 1694.0769\n",
      "test residual 1471.5327\n",
      "\n",
      "k:  3 ;  fold:  0\n",
      "train residual 1627.1642\n",
      "test residual 1416.612\n",
      "\n",
      "k:  3 ;  fold:  1\n",
      "train residual 1581.844\n",
      "test residual 1457.9668\n",
      "\n",
      "k:  3 ;  fold:  2\n",
      "train residual 1633.1164\n",
      "test residual 1397.7841\n",
      "\n",
      "k:  3 ;  fold:  3\n",
      "train residual 1596.3219\n",
      "test residual 1435.469\n",
      "\n",
      "k:  4 ;  fold:  0\n",
      "train residual 1579.97\n",
      "test residual 1414.8092\n",
      "\n",
      "k:  4 ;  fold:  1\n",
      "train residual 1535.3087\n",
      "test residual 1438.0591\n",
      "\n",
      "k:  4 ;  fold:  2\n",
      "train residual 1585.7068\n",
      "test residual 1394.8237\n",
      "\n",
      "k:  4 ;  fold:  3\n",
      "train residual 1546.5136\n",
      "test residual 1412.8556\n",
      "\n",
      "k:  5 ;  fold:  0\n",
      "train residual 1546.6095\n",
      "test residual 1409.3642\n",
      "\n",
      "k:  5 ;  fold:  1\n",
      "train residual 1508.0378\n",
      "test residual 1444.0499\n",
      "\n",
      "k:  5 ;  fold:  2\n",
      "train residual 1545.6905\n",
      "test residual 1374.4662\n",
      "\n",
      "k:  5 ;  fold:  3\n",
      "train residual 1516.0879\n",
      "test residual 1417.2554\n",
      "\n",
      "k:  6 ;  fold:  0\n",
      "train residual 1513.2382\n",
      "test residual 1381.6897\n",
      "\n",
      "k:  6 ;  fold:  1\n",
      "train residual 1481.5607\n",
      "test residual 1444.1233\n",
      "\n",
      "k:  6 ;  fold:  2\n",
      "train residual 1514.1667\n",
      "test residual 1371.3373\n",
      "\n",
      "k:  6 ;  fold:  3\n",
      "train residual 1494.6088\n",
      "test residual 1426.2846\n",
      "\n",
      "k:  7 ;  fold:  0\n",
      "train residual 1488.7519\n",
      "test residual 1375.6518\n",
      "\n",
      "k:  7 ;  fold:  1\n",
      "train residual 1458.5086\n",
      "test residual 1435.5543\n",
      "\n",
      "k:  7 ;  fold:  2\n",
      "train residual 1487.2031\n",
      "test residual 1367.2947\n",
      "\n",
      "k:  7 ;  fold:  3\n",
      "train residual 1469.5428\n",
      "test residual 1417.8283\n",
      "\n",
      "k:  8 ;  fold:  0\n",
      "train residual 1465.017\n",
      "test residual 1372.008\n",
      "\n",
      "k:  8 ;  fold:  1\n",
      "train residual 1435.0609\n",
      "test residual 1434.5844\n",
      "\n",
      "k:  8 ;  fold:  2\n",
      "train residual 1464.5381\n",
      "test residual 1363.4179\n",
      "\n",
      "k:  8 ;  fold:  3\n",
      "train residual 1445.701\n",
      "test residual 1414.0867\n",
      "\n",
      "k:  9 ;  fold:  0\n",
      "train residual 1449.94\n",
      "test residual 1395.6662\n",
      "\n",
      "k:  9 ;  fold:  1\n",
      "train residual 1413.3932\n",
      "test residual 1428.9961\n",
      "\n",
      "k:  9 ;  fold:  2\n",
      "train residual 1448.152\n",
      "test residual 1384.6344\n",
      "\n",
      "k:  9 ;  fold:  3\n",
      "train residual 1423.7332\n",
      "test residual 1406.7154\n",
      "\n",
      "k:  10 ;  fold:  0\n",
      "train residual 1432.1943\n",
      "test residual 1391.1841\n",
      "\n",
      "k:  10 ;  fold:  1\n",
      "train residual 1393.3042\n",
      "test residual 1424.4201\n",
      "\n",
      "k:  10 ;  fold:  2\n",
      "train residual 1429.9158\n",
      "test residual 1380.3149\n",
      "\n",
      "k:  10 ;  fold:  3\n",
      "train residual 1405.5788\n",
      "test residual 1400.7487\n",
      "\n",
      "k:  11 ;  fold:  0\n",
      "train residual 1412.785\n",
      "test residual 1390.5715\n",
      "\n",
      "k:  11 ;  fold:  1\n",
      "train residual 1375.0596\n",
      "test residual 1417.2233\n",
      "\n",
      "k:  11 ;  fold:  2\n",
      "train residual 1410.7016\n",
      "test residual 1379.5601\n",
      "\n",
      "k:  11 ;  fold:  3\n",
      "train residual 1389.3898\n",
      "test residual 1397.0048\n",
      "\n",
      "k:  12 ;  fold:  0\n",
      "train residual 1395.6363\n",
      "test residual 1388.8989\n",
      "\n",
      "k:  12 ;  fold:  1\n",
      "train residual 1360.6366\n",
      "test residual 1417.6959\n",
      "\n",
      "k:  12 ;  fold:  2\n",
      "train residual 1393.0306\n",
      "test residual 1377.6845\n",
      "\n",
      "k:  12 ;  fold:  3\n",
      "train residual 1370.5891\n",
      "test residual 1391.0051\n",
      "\n",
      "k:  13 ;  fold:  0\n",
      "train residual 1377.3541\n",
      "test residual 1380.7654\n",
      "\n",
      "k:  13 ;  fold:  1\n",
      "train residual 1344.4917\n",
      "test residual 1415.2601\n",
      "\n",
      "k:  13 ;  fold:  2\n",
      "train residual 1377.9818\n",
      "test residual 1373.9124\n",
      "\n",
      "k:  13 ;  fold:  3\n",
      "train residual 1358.3524\n",
      "test residual 1409.007\n",
      "\n",
      "k:  14 ;  fold:  0\n",
      "train residual 1362.802\n",
      "test residual 1378.3515\n",
      "\n",
      "k:  14 ;  fold:  1\n",
      "train residual 1332.7674\n",
      "test residual 1433.8729\n",
      "\n",
      "k:  14 ;  fold:  2\n",
      "train residual 1359.5603\n",
      "test residual 1367.3219\n",
      "\n",
      "k:  14 ;  fold:  3\n",
      "train residual 1343.8213\n",
      "test residual 1405.4902\n",
      "\n",
      "k:  15 ;  fold:  0\n",
      "train residual 1349.8671\n",
      "test residual 1379.0274\n",
      "\n",
      "k:  15 ;  fold:  1\n",
      "train residual 1319.2788\n",
      "test residual 1429.9993\n",
      "\n",
      "k:  15 ;  fold:  2\n",
      "train residual 1345.6355\n",
      "test residual 1365.934\n",
      "\n",
      "k:  15 ;  fold:  3\n",
      "train residual 1330.4147\n",
      "test residual 1404.0932\n",
      "\n",
      "k:  16 ;  fold:  0\n",
      "train residual 1337.9356\n",
      "test residual 1381.7093\n",
      "\n",
      "k:  16 ;  fold:  1\n",
      "train residual 1306.2258\n",
      "test residual 1426.7081\n",
      "\n",
      "k:  16 ;  fold:  2\n",
      "train residual 1332.1031\n",
      "test residual 1365.1327\n",
      "\n",
      "k:  16 ;  fold:  3\n",
      "train residual 1316.377\n",
      "test residual 1400.8884\n",
      "\n",
      "k:  17 ;  fold:  0\n",
      "train residual 1322.4966\n",
      "test residual 1377.072\n",
      "\n",
      "k:  17 ;  fold:  1\n",
      "train residual 1294.7876\n",
      "test residual 1425.5426\n",
      "\n",
      "k:  17 ;  fold:  2\n",
      "train residual 1320.3017\n",
      "test residual 1364.2375\n",
      "\n",
      "k:  17 ;  fold:  3\n",
      "train residual 1305.3623\n",
      "test residual 1401.0301\n",
      "\n",
      "k:  18 ;  fold:  0\n",
      "train residual 1310.9626\n",
      "test residual 1376.9753\n",
      "\n",
      "k:  18 ;  fold:  1\n",
      "train residual 1283.3947\n",
      "test residual 1425.2063\n",
      "\n",
      "k:  18 ;  fold:  2\n",
      "train residual 1303.7724\n",
      "test residual 1361.4936\n",
      "\n",
      "k:  18 ;  fold:  3\n",
      "train residual 1293.4558\n",
      "test residual 1400.0041\n",
      "\n",
      "k:  19 ;  fold:  0\n",
      "train residual 1299.2866\n",
      "test residual 1377.8717\n",
      "\n",
      "k:  19 ;  fold:  1\n",
      "train residual 1272.215\n",
      "test residual 1425.8643\n",
      "\n",
      "k:  19 ;  fold:  2\n",
      "train residual 1291.8677\n",
      "test residual 1361.2694\n",
      "\n",
      "k:  19 ;  fold:  3\n",
      "train residual 1282.0927\n",
      "test residual 1397.2888\n",
      "\n",
      "k:  20 ;  fold:  0\n",
      "train residual 1288.4097\n",
      "test residual 1375.9421\n",
      "\n",
      "k:  20 ;  fold:  1\n",
      "train residual 1263.6203\n",
      "test residual 1425.2783\n",
      "\n",
      "k:  20 ;  fold:  2\n",
      "train residual 1280.6114\n",
      "test residual 1360.1489\n",
      "\n",
      "k:  20 ;  fold:  3\n",
      "train residual 1270.9613\n",
      "test residual 1397.2668\n",
      "\n",
      "k:  21 ;  fold:  0\n",
      "train residual 1278.9174\n",
      "test residual 1373.835\n",
      "\n",
      "k:  21 ;  fold:  1\n",
      "train residual 1251.7759\n",
      "test residual 1423.9232\n",
      "\n",
      "k:  21 ;  fold:  2\n",
      "train residual 1270.5216\n",
      "test residual 1367.8678\n",
      "\n",
      "k:  21 ;  fold:  3\n",
      "train residual 1262.3891\n",
      "test residual 1399.9132\n",
      "\n",
      "k:  22 ;  fold:  0\n",
      "train residual 1270.451\n",
      "test residual 1392.0149\n",
      "\n",
      "k:  22 ;  fold:  1\n",
      "train residual 1242.0566\n",
      "test residual 1421.5048\n",
      "\n",
      "k:  22 ;  fold:  2\n",
      "train residual 1261.7861\n",
      "test residual 1368.9321\n",
      "\n",
      "k:  22 ;  fold:  3\n",
      "train residual 1253.3495\n",
      "test residual 1401.8732\n",
      "\n",
      "k:  23 ;  fold:  0\n",
      "train residual 1260.7047\n",
      "test residual 1391.3616\n",
      "\n",
      "k:  23 ;  fold:  1\n",
      "train residual 1235.4811\n",
      "test residual 1432.1839\n",
      "\n",
      "k:  23 ;  fold:  2\n",
      "train residual 1252.3341\n",
      "test residual 1368.2511\n",
      "\n",
      "k:  23 ;  fold:  3\n",
      "train residual 1244.4521\n",
      "test residual 1401.4837\n",
      "\n",
      "k:  24 ;  fold:  0\n",
      "train residual 1251.2666\n",
      "test residual 1390.9123\n",
      "\n",
      "k:  24 ;  fold:  1\n",
      "train residual 1225.2048\n",
      "test residual 1423.4689\n",
      "\n",
      "k:  24 ;  fold:  2\n",
      "train residual 1241.3435\n",
      "test residual 1367.5179\n",
      "\n",
      "k:  24 ;  fold:  3\n",
      "train residual 1236.1432\n",
      "test residual 1402.1691\n",
      "\n",
      "k:  25 ;  fold:  0\n",
      "train residual 1241.5844\n",
      "test residual 1390.0848\n",
      "\n",
      "k:  25 ;  fold:  1\n",
      "train residual 1215.3682\n",
      "test residual 1422.1493\n",
      "\n",
      "k:  25 ;  fold:  2\n",
      "train residual 1233.2777\n",
      "test residual 1380.7872\n",
      "\n",
      "k:  25 ;  fold:  3\n",
      "train residual 1227.8391\n",
      "test residual 1411.4239\n",
      "\n",
      "k:  26 ;  fold:  0\n",
      "train residual 1231.2286\n",
      "test residual 1390.883\n",
      "\n",
      "k:  26 ;  fold:  1\n",
      "train residual 1206.7013\n",
      "test residual 1421.791\n",
      "\n",
      "k:  26 ;  fold:  2\n",
      "train residual 1223.2863\n",
      "test residual 1379.4047\n",
      "\n",
      "k:  26 ;  fold:  3\n",
      "train residual 1218.9438\n",
      "test residual 1401.5944\n",
      "\n",
      "k:  27 ;  fold:  0\n",
      "train residual 1221.5912\n",
      "test residual 1390.3532\n",
      "\n",
      "k:  27 ;  fold:  1\n",
      "train residual 1199.576\n",
      "test residual 1425.0109\n",
      "\n",
      "k:  27 ;  fold:  2\n",
      "train residual 1214.3895\n",
      "test residual 1378.8247\n",
      "\n",
      "k:  27 ;  fold:  3\n",
      "train residual 1210.5293\n",
      "test residual 1410.8755\n",
      "\n",
      "k:  28 ;  fold:  0\n",
      "train residual 1213.511\n",
      "test residual 1390.2125\n",
      "\n",
      "k:  28 ;  fold:  1\n",
      "train residual 1192.8157\n",
      "test residual 1425.5598\n",
      "\n",
      "k:  28 ;  fold:  2\n",
      "train residual 1206.1373\n",
      "test residual 1378.0698\n",
      "\n",
      "k:  28 ;  fold:  3\n",
      "train residual 1200.0358\n",
      "test residual 1408.8628\n",
      "\n",
      "k:  29 ;  fold:  0\n",
      "train residual 1209.1725\n",
      "test residual 1404.1497\n",
      "\n",
      "k:  29 ;  fold:  1\n",
      "train residual 1187.2409\n",
      "test residual 1433.7083\n",
      "\n",
      "k:  29 ;  fold:  2\n",
      "train residual 1197.997\n",
      "test residual 1384.2919\n",
      "\n",
      "k:  29 ;  fold:  3\n",
      "train residual 1192.4381\n",
      "test residual 1413.0821\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run multiple k and capture results\n",
    "\n",
    "n_iter = 5000\n",
    "k_tup_list = []\n",
    "data = tfidf + 1\n",
    "k_min = 2\n",
    "for k in range(k_min,30):     \n",
    "\n",
    "    train_mean, test_mean = nmf_crossval(data, k, max_iter=n_iter)\n",
    "    k_tup = (train_mean, test_mean, k)\n",
    "    k_tup_list.append(k_tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extracting the best run for K in range 2-30, step 1\n",
    "test_err = [tup[1] for tup in k_tup_list]\n",
    "min_test_err = np.min(test_err)\n",
    "min_test_err_idx = test_err.index(min_test_err)\n",
    "best_k = min_test_err_idx + k_min\n",
    "\n",
    "test_vals = [x[1] for x in k_tup_list]\n",
    "train_vals = [x[0] for x in k_tup_list]\n",
    "# plot\n",
    "plt.plot(np.arange(2,len(train_vals)+2), train_vals, label=\"train\")\n",
    "plt.plot(np.arange(2,len(test_vals)+2), test_vals, label=\"test\")\n",
    "plt.title(\"Training and test error for various K\")\n",
    "plt.xlabel(\"K\")\n",
    "plt.ylabel(\"error\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best (lowest) error is 1389.6590211863545, with 20 latent features\n"
     ]
    }
   ],
   "source": [
    "print(\"Best (lowest) error is {}, with {} latent features\".format(min_test_err, best_k))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1db9c0cf1b6263b629717abeeeeab122170cf719ce341bc0e2e624706511d5f7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('.venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
